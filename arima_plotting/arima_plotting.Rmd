---
title: 'Arima model evaluation'
author: 'Jay Achar'
date: "`r Sys.time()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 2
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE)
suppressMessages(library(tidyverse))
library(here)
library(forecast)
library(ggdist)
library(patchwork)
```

```{r source-functions}
files <- list.files(here("R"))
walk(files,
     ~source(here("R", .x)))
```

# Important

I've noticed these parsing errors when reading in the data. I haven't looked through
them, but this would be important to check before finalizing things.

```{r read-in-data}
raw <- read_data("notifications")
```

## Define constants

It's often helpful to define standard variables that shouldn't change
at the top of your script. It's then easy to change later if you want to
re-run an analysis and also to see quickly what has been applied.

```{r define-constants-functions}
# returns a list of constants
const <- constants()
```

# Data preparation

I tend to do all of my data preparation at the beginning. This might require
that you move later data cleaning tasks to this area, but having everything
mixed up is hard to work through for others looking at your code.

Here I've created a list `full_data` which includes to data frames  - `region`
and `hbc`. These represent aggregate notifications counts by age group and year
for WHO regions and high burden countries respectively.

```{r generate-data-structures}
dd <- prepare_data(raw, const = const)

region_df <- dd$long$region %>% 
  filter(year >= 2014,
         age_group != "014") %>%  
  mutate(age_group = factor(age_group, levels = c("04", "514", "15plus"),
                            ordered = TRUE)) %>% 
  group_by(g_whoregion, age_group, year) %>% 
  summarise(cases = sum(cases, na.rm = TRUE)) %>% 
  select(location = g_whoregion, everything())

hbc_df <- dd$long$country %>%
  filter(iso3 %in% const$high_burden) %>%
  filter(year >= 2014,
         age_group != "014") %>%
  mutate(age_group = factor(
    age_group,
    levels = c("04", "514", "15plus"),
    ordered = TRUE
  )) %>%
  group_by(iso3, age_group, year) %>%
  summarise(cases = sum(cases, na.rm = TRUE)) %>% 
  select(location = iso3, everything())

full <- list(
  region = region_df,
  hbc = hbc_df
)

head(full)
```

## Create training data set

Next, we create a model training set by removing 2019 and 2020 values from our
full data. Models will be evaluated on their accuracy to the 2019 data.

```{r training-set}
training <- full %>% 
  map(.f = ~ dplyr::filter(.x, ! year %in% c(2019, 2020)) %>% 
        group_by(location, age_group) %>% 
        nest())

head(training)

```

## Create training time-series data

Let's convert each country/region to a time-series object and create
a `combined` tibble which includes all HB countries and WHO regions for
ease of evaluation later. 


```{r training-ts}

training_ts <- training %>% 
  map(~ mutate(.x,
               ts = map(data, function(series) {
                 select(series, -year) %>% 
                   ts(start = 2014, frequency = 1)
               })))

training_ts$combined <- bind_rows(training_ts, .id = 'dataset')

head(training_ts)
```

# Evaluate different Arima models

## Calculate metrics

Using a range of arima parameters, we will calculate the
`r paste0(toupper(const$model_performance_metrics), collapse = ", ")` for each model
then plot them against each other. Each model is fitted to the data for each
`location` and `age_group` combination before the model metric is calculated.
The mean of these metric values is presented here since some models do not
converge on some data sets. Note that a training set is used here, so 2019
data points are not included.

```{r calculate-metrics}
metrics <- map(
  training_ts,
  function(df) {
    map(const$model_performance_metrics, function(metric) {
    const$model_params %>%
      mutate(models = pmap(
        list(a, b, c, drift),
        ~ calculate_model_fit(df = df,
                              c(..1, ..2, ..3),
                              drift = ..4)
      )) %>%
      mutate(
        model_count = map_int(models, length),
        sum = map_dbl(models, ~ extract_model_fit(.x, param = metric)),
        mean = sum / model_count
      ) %>%
      arrange(mean)
  }) %>% set_names(const$model_performance_metrics) %>% bind_rows(.id = "metric")
  
  } 
)

metrics <- map(metrics,
    .f = function(dataset) {
      dataset %>% 
        mutate(d_lab = ifelse(drift == TRUE, "T", "F")) %>% 
        mutate(model_spec = factor(str_c(a, b, c, d_lab))) %>% 
        select(metric, model_spec, everything(), -d_lab)
    })
  
```

## Plot mean model metrics

```{r metrics-region}
metrics$region %>% 
  filter(model_count == 18) %>% 
  filter(mean < 120) %>% 
  ggplot(aes(x = model_spec,
         y = mean,
         color = metric)) + 
  geom_point() +
  coord_flip() +
  theme_minimal() +
  labs(title = "WHO regional training data",
       subtitle = "Only where all 18 models converged")
```

```{r metrics-hbc}
metrics$hbc %>% 
  filter(model_count > 75) %>% 
  filter(mean < 120) %>% 
  ggplot(aes(x = model_spec,
         y = mean,
         color = metric)) + 
  geom_point() +
  coord_flip() +
  theme_minimal() +
  labs(title = "HBC training data",
       subtitle = "Only where >75/90 models converged")
```

```{r metrics-combined}
metrics$combined %>% 
  filter(model_count > 90) %>% 
  filter(mean < 120) %>% 
  ggplot(aes(x = model_spec,
         y = mean,
         color = metric)) + 
  geom_point() +
  coord_flip() +
  theme_minimal() +
  labs(title = "Combined WHO regional & HBC training data",
       subtitle = "Only where >90/108 models converged")
```

## Model selection

Let's take the 10 models with the lowest mean cAIC based on the combined WHO
and HBC training data sets:

```{r model-selection}
combined_models <- metrics$combined %>% 
  filter(metric == "aicc") %>% 
  filter(model_count > 90) %>% 
  arrange(mean) %>%
  slice_head(n = 10)

combined_models 
```

```{r plot-model-selection }
ggplot(combined_models,
       aes(x = model_spec,
           y = mean)) +
  geom_point() +
  scale_y_continuous(limits = c(-100, 100)) +
  coord_flip() +
  theme_minimal() +
  labs(title = "Best cAICC for models trained on combined data",
       subtitle = "Includes models which converged on >90/108 data sets", 
       x = "Model parameters",
       y = "Mean cAIC")
```

## Evaluate model accuracy

Now that we have 10 candidates with the lowest cAIC, lets compare their 2019
predictions with those that were observed. 

What is the best way to assess accuracy here? 

I've used mean difference across `location` and `age_group` conbinations here.

Observations: 

  - Notice how only 2 models converge across all groups
  - Mean difference is reasonably similar across models
  - See below that the distribution of differences is consistent and
  really very close to 0

```{r model-accuracy, message = FALSE, warning = FALSE}

# prepare observed notifications for 2019
observed <- prepare_actual_notifications(full, training_ts$combined)

# uses mean difference as discriminator
prediction_comparison <- combined_models %>%
  mutate(
    prediction = pmap(
      list(a, b, c, drift),
      ~ evaluate_predictions(observed_data = observed,
                             order = c(..1, ..2, ..3),
                             drift = ..4, 
                             function(observed, predicted) {
                                   predicted - observed
                                 }))) %>% 
  select(model_spec,
         prediction, model_count) %>% 
  unnest()
```

```{r model-accuracy-plot-absolute-mean}
mean_difference <- prediction_comparison %>%
  mutate(absolute_diff = abs(prediction)) %>% 
  group_by(model_spec, model_count) %>% 
  summarise(mean = mean(prediction, na.rm = TRUE),
            abs_mean = mean(absolute_diff, na.rm = TRUE))



absolute_mean_diff_plot <-  mean_difference %>% 
  ggplot(
    aes(x = model_spec,
        y = abs_mean,
        color = model_count == 108)
  ) +
  geom_point(aes(size = model_count)) +
  coord_flip() + 
  theme_minimal() + 
  labs(title = "Predictive accuracy",
       subtitle = "Absolute mean difference between predicted 2019 and observed 2019",
       y = "Absolute mean difference in notifications")

```

```{r model-accuracy-plot-mean, fig.height=7, fig.width=14}
mean_diff_plot <-  mean_difference %>% 
  ggplot(
    aes(x = model_spec,
        y = mean,
        color = model_count == 108)
  ) +
  geom_point(aes(size = model_count)) +
  coord_flip() + 
  theme_minimal() + 
  labs(title = "Predictive accuracy",
       subtitle = "Mean difference between predicted 2019 and observed 2019",
       y = "Mean difference in notifications")


absolute_mean_diff_plot / mean_diff_plot + plot_layout(guides = "collect")
```

```{r model-accuracy-variance }

prediction_comparison %>% 
  group_by(model_spec) %>% 
  summarise(mean = mean(prediction, na.rm = TRUE),
            p5 = quantile(prediction, 0.05, na.rm = TRUE),
            p50 = quantile(prediction, 0.5, na.rm = TRUE),
            p95 = quantile(prediction, 0.95, na.rm = TRUE))


prediction_comparison %>% 
  filter(!is.na(prediction)) %>% 
  ggplot(aes(x = prediction)) +
  geom_density(alpha = 0.2) +
  facet_grid(model_spec ~ .) +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(title = "Distribution of prediction error",
       subtitle = "Difference between 2019 predicted and observed notifications across all age groups",
       x = "Prediction error",
       y = "Density")
```

## Example

Let's review one model specification to visualise evaluate the prediction for
2019 notifications across countries/regions and age groups.

Notice how the predictions are fairly reasonble across all of the groups.

Notifications are

Model 2, 0, 0 with drift:

```{r example-comparison, warning = FALSE, fig.height=12, fig.width=7}
 example_predicted <- forecast_next_year(training_ts$combined,
                                  order = c(2, 0, 0), drift = TRUE)
 
 example_df <- example_predicted %>% 
   right_join(observed, by = c("location", "age_group")) %>% 
   select(location, age_group, observed, predicted = forecast) %>% 
   pivot_longer(cols = c(observed, predicted),
                names_to = "type", values_to = "notifications")
 
 example_df %>% 
   ggplot(aes(x = location,
              y = notifications,
              color = type)) + 
   geom_point() +
   facet_wrap(age_group ~ ., scales = "free_x") + 
   coord_flip() +
   theme_minimal() +
   labs(title = "Predicted against observed 2019 notifciations",
        subtitle = "(2, 0, 2) with drift",
        x = "Location",
        y = "Notifications",
        color = "Type",
        caption = "Beware of the different x-axis scales") +
   theme(
     legend.position = "bottom"
   )
 
```

